---
title: "Biostat 203B Homework 1"
subtitle: Due Jan 24, 2025 @ 11:59PM
author: Luke Hodges 906182810
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
    link-external-icon: true
    link-external-newwindow: true
---

Display machine information for reproducibility:
```{r}
#| eval: true
sessionInfo()
```

## Q1. Git/GitHub

**No handwritten homework reports are accepted for this course.** We work with Git and GitHub. Efficient and abundant use of Git, e.g., frequent and well-documented commits, is an important criterion for grading your homework.

1. Apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email. You'll get GitHub Pro account for free (unlimited public and private repositories).

2. Create a **private** repository `biostat-203b-2025-winter` and add `Hua-Zhou` and TA team (`Tomoki-Okuno` for Lec 1; `parsajamshidian` and `BowenZhang2001` for Lec 82) as your collaborators with write permission.

3. Top directories of the repository should be `hw1`, `hw2`, ... Maintain two branches `main` and `develop`. The `develop` branch will be your main playground, the place where you develop solution (code) to homework problems and write up report. The `main` branch will be your presentation area. Submit your homework files (Quarto file `qmd`, `html` file converted by Quarto, all code and extra data sets to reproduce results) in the `main` branch.

4. After each homework due date, course reader and instructor will check out your `main` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your `hw1` submission after deadline, penalty points will be deducted for late submission.

5. After this course, you can make this repository public and use it to demonstrate your skill sets on job market.

**Solution** Done.

## Q2. Data ethics training
This exercise (and later in this course) uses the MIMIC-IV data v3.1, a freely accessible critical care database developed by the MIT Lab for Computational Physiology. Follow the instructions at https://mimic.mit.edu/docs/gettingstarted/ to (1) complete the CITI Data or Specimens Only Research course and (2) obtain the PhysioNet credential for using the MIMIC-IV data. Display the verification links to your completion report and completion certificate here. You must complete Q2 before working on the remaining questions. (Hint: The CITI training takes a few hours and the PhysioNet credentialing takes a couple days; do not leave it to the last minute.)

**Solution** Here is the [Completion Report](https://www.citiprogram.org/verify/?ke969a4f4-b4ad-4066-bb3f-8febb17a314d-67306596) and my [Completion Certificate](https://www.citiprogram.org/verify/?w1ebe72ea-777e-4eee-8de7-0e0dd1c611a3-67306596) of my CITI Training. 

## Q3. Linux Shell Commands
Make the MIMIC-IV v3.1 data available at location ~/mimic. The output of the ls -l ~/mimic command should be similar to the below (from my laptop).

# content of mimic folder
```{bash}
#| eval: true
#content of mimic folder
ls -l ~/mimic/
```
Refer to the documentation https://physionet.org/content/mimiciv/3.1/ for details of data files. Do not put these data files into Git; they are big. Do not copy them into your directory. Do not decompress the gz data files. These create unnecessary big files and are not big-data-friendly practices. Read from the data folder ~/mimic directly in following exercises.

Use Bash commands to answer following questions.

**Solution** I downloaded the Mimic_IV v3.1 data and it is available under the `~/mimic` folder as requested. 

2. Display the contents in the folders hosp and icu using Bash command ls -l. Why are these data files distributed as .csv.gz files instead of .csv (comma separated values) files? Read the page https://mimic.mit.edu/docs/iv/ to understand what’s in each folder.

**Solution** Here is the content of the `hosp` folder

```{bash}
ls -l ~/mimic/hosp/
```

**Solution** Here is the content of the `icu` folder

```{bash}
ls -l ~/mimic/icu/
```

**These data files were distrbuted as `gz` files because they are giant files and allows them to be downloaded and distributed faster and easier.**

3. Briefly describe what Bash commands zcat, zless, zmore, and zgrep do.

**Solution** Zcat decompresses a compressed file (.gz).

**Solution** Zmore views compressed files, but cannot navigate as easily

**Solution** Zless views compressed files with easy navigation line by line

**Solution** Zgrep searches compressed files for terms and returns with matching lines. 

4. (Looping in Bash) What’s the output of the following bash script?

**Solution** Here is the output

```{bash}
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  ls -l $datafile
done
```

Display the number of lines in each data file using a similar loop. (Hint: combine linux commands zcat < and wc -l.)

**Solution** Here are the lines in admissions file

```{bash}
for datafile in ~/mimic/hosp/admissions.csv.gz
do
zcat < $datafile | wc -l
done
```

**Solution** Here are the lines in the labevents file

```{bash}
for datafile in ~/mimic/hosp/labevents.csv.gz
do
zcat < $datafile | wc -l
done
```

**Solution** Here are the lines in the patients file

```{bash}
for datafile in ~/mimic/hosp/patients.csv.gz
do
zcat < $datafile | wc -l
done
```

5. Display the first few lines of admissions.csv.gz. How many rows are in this data file, excluding the header line? Each hadm_id identifies a hospitalization. How many hospitalizations are in this data file? How many unique patients (identified by subject_id) are in this data file? Do they match the number of patients listed in the patients.csv.gz file? (Hint: combine Linux commands zcat <, head/tail, awk, sort, uniq, wc, and so on.)

**Solution** Here are the first few lines of admissions.csv.gz

```{bash}
for datafile in ~/mimic/hosp/admissions.csv.gz
do
zcat < $datafile | head
done
```

**Solution** Counting the total number of rows excluding the header

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | wc -l
```

**Solution** Number of unique hospitalizations
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $2}' |
sort |
uniq |
wc -l
```
The number of rows is the number of unique hospitalizations. 

Peek the first few lines of 'patients.csv.gz'
```{bash}
zcat < ~/mimic/hosp/patients.csv.gz | head
```
The number of unique patients in this file is: 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```

This should match the number in the patients file 

```{bash}
zcat < ~/mimic/hosp/patients.csv.gz | 
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```

The total number of unique patients in the admissions file is less. 

6. What are the possible values taken by each of the variable admission_type, admission_location, insurance, and ethnicity? Also report the count for each unique value of these variables in decreasing order. (Hint: combine Linux commands zcat, head/tail, awk, uniq -c, wc, sort, and so on; skip the header line.)

**Solution** You need to first examine the admissions file. 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | head
```
Admission type is the sixth column so the possible values include: 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $6}' |
sort |
uniq |
wc -l
```
So there are nine different admission types. 

For Admissions location: 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $8}' |
sort |
uniq |
wc -l
```
There are twelve different admission locations. 

For insurance:

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $10}' |
sort |
uniq |
wc -l
```
There are six insurance

For ethnicity:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $13}' |
sort |
uniq |
wc -l
```
There are 33 ethnicities.

7. The icusays.csv.gz file contains all the ICU stays during the study period. How many ICU stays, identified by stay_id, are in this data file? How many unique patients, identified by subject_id, are in this data file?

**Solution**

To start, we must look into the icusays.csv.gz file 

```{bash}
zcat < ~/mimic/icu/icustays.csv.gz | head
```

The number of ICU stays identified by stay_id

```{bash}
zcat < ~/mimic/icu/icustays.csv.gz | 
tail -n +2 |
awk -F, '{print $3}' |
sort |
uniq |
wc -l
```
There are 94458 ICU stays identified by the stay_id

The number of patients identified by subject_id

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```
There are 223452 patients identified by subject_id

8. To compress, or not to compress. That’s the question. Let’s focus on the big data file labevents.csv.gz. Compare compressed gz file size to the uncompressed file size. Compare the run times of zcat < ~/mimic/labevents.csv.gz | wc -l versus wc -l labevents.csv. Discuss the trade off between storage and speed for big data files. (Hint: gzip -dk < FILENAME.gz > ./FILENAME. Remember to delete the large labevents.csv file after the exercise.)

Checking file size for the compressed version
```{bash}
ls -lh ~/mimic/hosp/labevents.csv.gz
```

This file is 2.4G 

Checking file size for the uncompressed version
```{bash}
ls -lh ~/mimic/hosp/labevents.csv
```

This file is 17G

Comparing the run times of the compressed and uncompressed 

For Compressed

```{bash}
time zcat < ~/mimic/hosp/labevents.csv.gz | wc -l
```
For Uncompressed 

```{bash}
time wc -l ~/mimic/hosp/labevents.csv
```

This shows that compressed is slower than uncompressed. 

This is because compressed has to go through the decompressing of the file 

In other words, compressed saves storage, but slows time

Uncompressed takes up storage, but is faster. 

## Q4. Who’s popular in Price and Prejudice

1. You and your friend just have finished reading Pride and Prejudice by Jane Austen. Among the four main characters in the book, Elizabeth, Jane, Lydia, and Darcy, your friend thinks that Darcy was the most mentioned. You, however, are certain it was Elizabeth. Obtain the full text of the novel from http://www.gutenberg.org/cache/epub/42671/pg42671.txt and save to your local folder.

```{bash}
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
for char in Elizabeth Jane Lydia Darcy
do
  echo $char:
  grep -n "$char" pg42671.txt | wc -l
done
```

Explain what wget -nc does. Do not put this text file pg42671.txt in Git. Complete the following loop to tabulate the number of times each of the four characters is mentioned using Linux commands.

Wget -nc downloads the file, but doers not replicate if it already is downloaded. 

2. What’s the difference between the following two commands?

```{bash}
echo 'hello, world' > test1.txt
```
and

```{bash}
echo 'hello, world' >> test2.txt
```

The first command directs the output to the test1.txt, saying 'hello, world'. 

The second command appends (adds it) to a file named test2.txt. 

Using your favorite text editor (e.g., vi), type the following and save the file as middle.sh:

#!/bin/sh
# Select lines from the middle of a file.
# Usage: bash middle.sh filename end_line num_lines
head -n "$2" "$1" | tail -n "$3"

Using chmod to make the file executable by the owner, and run

The middle.sh is set to my desktop. The command chmod 751 gives executive function and access to the user. 
```{bash}
chmod 751 /Users/lukehodges/desktop/middle.sh
```

```{bash}
/Users/lukehodges/desktop/middle.sh pg42671.txt 20 5
```
Explain the output. Explain the meaning of "$1", "$2", and "$3" in this shell script. Why do we need the first line of the shell script?

**The output is the 20th line of the file pg42671.txt and the following 5 lines. The "$1" is the first argument, which is the file name. The "$2" is the second argument, which is the line. The "$3" is the third argument, which is the number of lines. The first line of the shell script is needed to tell the computer what shell to use to run the script.**

## Q5. More fun with Linux
Try following commands in Bash and interpret the results: cal, cal 2025, cal 9 1752 (anything unusual?), date, hostname, arch, uname -a, uptime, who am i, who, w, id, last | head, echo {con,pre}{sent,fer}{s,ed}, time sleep 5, history | tail.

```{bash}
cal
```
**This makes a calendar of the current month of the current year**

```{bash}
cal 2025
```
**This makes an annual calendar**

```{bash}
cal 9 1752
```
**This makes a calendar for September, 1752. It skips 3-13.**

```{bash}
date
```
**Gives you current date and time**

```{bash}
hostname
```
**Gives name of local computer**

```{bash}
arch
```
**Indicates the architecture of the computer (32-bit system)**

```{bash}
uname -a
```
**Gives you more details of the local computer system, like version. 

```{bash}
uptime
```
**Shows you how long the computer has been running**

```{bash}
who am i
```
**Gives you name of the user with date and time**

```{bash}
who
```
**Gives you who is currently using the local computer. 

```{bash}
w
```
**Seems to summarize and add from the information above.**

```{bash}
id
```
**Displays individual and group IDs**

```{bash}
last | head
```
**Displays the first few lines of people who have logged in and when**

```{bash}
echo {con,pre}{sent,fer}{s,ed}
```
**Matches every possible combination in the set in order**

```{bash}
time sleep 5
```
**Shows how long the sleep command takes to run**

```{bash}
history | tail
```
**Displays the last several commands executed**

## Q6. Book
Git clone the repository https://github.com/christophergandrud/Rep-Res-Book for the book Reproducible Research with R and RStudio to your local machine. Do not put this repository within your homework repository biostat-203b-2025-winter.

Open the project by clicking rep-res-3rd-edition.Rproj and compile the book by clicking Build Book in the Build panel of RStudio. (Hint: I was able to build git_book and epub_book directly. For pdf_book, I needed to add a line \usepackage{hyperref} to the file Rep-Res-Book/rep-res-3rd-edition/latex/preabmle.tex.)

The point of this exercise is (1) to obtain the book for free and (2) to see an example how a complicated project such as a book can be organized in a reproducible way. Use sudo apt install PKGNAME to install required Ubuntu packages and tlmgr install PKGNAME to install missing TexLive packages.

For grading purpose, include a screenshot of Section 4.1.5 of the book here.

**Solution** Here is the [picture](https://drive.google.com/file/d/15_cgNaiddl9u8btlhf_dY5PF1whKVtRp/view) of section 4.1.5.
