---
title: "Biostat 203B Homework 1"
subtitle: Due Jan 24, 2025 @ 11:59PM
author: Luke Hodges nd UID
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
    link-external-icon: true
    link-external-newwindow: true
---

Display machine information for reproducibility:
```{r}
#| eval: true
sessionInfo()
```

## Q1. Git/GitHub

**No handwritten homework reports are accepted for this course.** We work with Git and GitHub. Efficient and abundant use of Git, e.g., frequent and well-documented commits, is an important criterion for grading your homework.

1. Apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email. You'll get GitHub Pro account for free (unlimited public and private repositories).

2. Create a **private** repository `biostat-203b-2025-winter` and add `Hua-Zhou` and TA team (`Tomoki-Okuno` for Lec 1; `parsajamshidian` and `BowenZhang2001` for Lec 82) as your collaborators with write permission.

3. Top directories of the repository should be `hw1`, `hw2`, ... Maintain two branches `main` and `develop`. The `develop` branch will be your main playground, the place where you develop solution (code) to homework problems and write up report. The `main` branch will be your presentation area. Submit your homework files (Quarto file `qmd`, `html` file converted by Quarto, all code and extra data sets to reproduce results) in the `main` branch.

4. After each homework due date, course reader and instructor will check out your `main` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your `hw1` submission after deadline, penalty points will be deducted for late submission.

5. After this course, you can make this repository public and use it to demonstrate your skill sets on job market.

**Solution** Done.

Q2. Data ethics training
This exercise (and later in this course) uses the MIMIC-IV data v3.1, a freely accessible critical care database developed by the MIT Lab for Computational Physiology. Follow the instructions at https://mimic.mit.edu/docs/gettingstarted/ to (1) complete the CITI Data or Specimens Only Research course and (2) obtain the PhysioNet credential for using the MIMIC-IV data. Display the verification links to your completion report and completion certificate here. You must complete Q2 before working on the remaining questions. (Hint: The CITI training takes a few hours and the PhysioNet credentialing takes a couple days; do not leave it to the last minute.)

**Solution** Here is the [Completion Report](https://www.citiprogram.org/verify/?ke969a4f4-b4ad-4066-bb3f-8febb17a314d-67306596) and my [Completion Certificate](https://www.citiprogram.org/verify/?w1ebe72ea-777e-4eee-8de7-0e0dd1c611a3-67306596) of my CITI Training. 

Q3. Linux Shell Commands
Make the MIMIC-IV v3.1 data available at location ~/mimic. The output of the ls -l ~/mimic command should be similar to the below (from my laptop).

# content of mimic folder
```{bash}
#| eval: true
#content of mimic folder
ls -l ~/mimic/
```

Refer to the documentation https://physionet.org/content/mimiciv/3.1/ for details of data files. Do not put these data files into Git; they are big. Do not copy them into your directory. Do not decompress the gz data files. These create unnecessary big files and are not big-data-friendly practices. Read from the data folder ~/mimic directly in following exercises.

Use Bash commands to answer following questions.

**Solution** I downloaded the Mimic_IV v3.1 data and it is available under the `~/mimic` folder as requested. 

2. Display the contents in the folders hosp and icu using Bash command ls -l. Why are these data files distributed as .csv.gz files instead of .csv (comma separated values) files? Read the page https://mimic.mit.edu/docs/iv/ to understand what’s in each folder.

**Solution** Here is the content of the `hosp` folder

```{bash}
ls -l ~/mimic/hosp/
```

**Solution** Here is the content of the `icu` folder

```{bash}
ls -l ~/mimic/icu/
```

**These data files were distrbuted as `gz` files because they are giant files and allows them to be downloaded and distributed faster and easier.**

3. Briefly describe what Bash commands zcat, zless, zmore, and zgrep do.

**Solution** Zcat will compress from standard input or decompress to standard output.

**Solution** Zmore views compressed files, but cannot navigate as easily

**Solution** Zless views compressed files with easy navigation line by line

**Solution** Zgrep searches compressed files for terms and returns with matching lines. 

4. (Looping in Bash) What’s the output of the following bash script?


**Solution** Here is the output

```{bash}
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  ls -l $datafile
done
```

Display the number of lines in each data file using a similar loop. (Hint: combine linux commands zcat < and wc -l.)

**Solution** Here are the lines in admissions file

```{bash}
for datafile in ~/mimic/hosp/admissions.csv.gz
do
zcat < $datafile | wc -l
done
```

**Solution** Here are the lines in the labevents file

```{bash}
for datafile in ~/mimic/hosp/labevents.csv.gz
do
zcat < $datafile | wc -l
done
```

**Solution** Here are the lines in the patients file

```{bash}
for datafile in ~/mimic/hosp/patients.csv.gz
do
zcat < $datafile | wc -l
done
```

5. Display the first few lines of admissions.csv.gz. How many rows are in this data file, excluding the header line? Each hadm_id identifies a hospitalization. How many hospitalizations are in this data file? How many unique patients (identified by subject_id) are in this data file? Do they match the number of patients listed in the patients.csv.gz file? (Hint: combine Linux commands zcat <, head/tail, awk, sort, uniq, wc, and so on.)

**Solution** Here are the first few lines of admissions.csv.gz

```{bash}
for datafile in ~/mimic/hosp/admissions.csv.gz
do
zcat < $datafile | head
done
```

**Solution** Counting the total number of rows excluding the header

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | wc -l
```

**Solution** Number of unique hospitalizations
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $2}' |
sort |
uniq |
wc -l
```
The number of rows is the number of unique hospitalizations. 

Peek the first few lines of 'patients.csv.gz'
```{bash}
zcat < ~/mimic/hosp/patients.csv.gz | head
```

The number of unique patients in this file is: 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```

This should match the number in the patients file 

```{bash}
zcat < ~/mimic/hosp/patients.csv.gz | 
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```
The total number of unique patients in the admissions file is less. 

6. What are the possible values taken by each of the variable admission_type, admission_location, insurance, and ethnicity? Also report the count for each unique value of these variables in decreasing order. (Hint: combine Linux commands zcat, head/tail, awk, uniq -c, wc, sort, and so on; skip the header line.)

**Solution** You need to first examine the admissions file. 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | head
```
Admission type is the sixth column so the possible values include: 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $6}' |
sort |
uniq |
wc -l
```
So there are nine different admission types. 

For Admissions location, 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $8}' |
sort |
uniq |
wc -l
```
There are twelve different admission types. 

For insurance, 

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $10}' |
sort |
uniq |
wc -l
```
There are six insurance

For ethnicity
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $13}' |
sort |
uniq |
wc -l
```
There are 33 ethnicities.

7. The icusays.csv.gz file contains all the ICU stays during the study period. How many ICU stays, identified by stay_id, are in this data file? How many unique patients, identified by subject_id, are in this data file?

**Solution**

To start, we must peek into the icusays.csv.gz file 

```{bash}
zcat < ~/mimic/icu/icustays.csv.gz | head
```

The number of ICU stays identified by stay_id

```{bash}
zcat < ~/mimic/icu/icustays.csv.gz | 
tail -n +2 |
awk -F, '{print $3}' |
sort |
uniq |
wc -l
```
There are 94458 ICU stays identified by the stay_id

The number of patients identified by subject_id

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```
There are 223452 patients identified by subject_id

8. To compress, or not to compress. That’s the question. Let’s focus on the big data file labevents.csv.gz. Compare compressed gz file size to the uncompressed file size. Compare the run times of zcat < ~/mimic/labevents.csv.gz | wc -l versus wc -l labevents.csv. Discuss the trade off between storage and speed for big data files. (Hint: gzip -dk < FILENAME.gz > ./FILENAME. Remember to delete the large labevents.csv file after the exercise.)

Checking file size for the compressed version
```{bash}
ls -lh ~/mimic/hosp/labevents.csv.gz
```
This file is 2.4G 
Checking file size for the uncompressed version
```{bash}
ls -lh ~/mimic/hosp/labevents.csv
```
This file is 17G

Comparing the run times of the compressed and uncompressed 

For Compressed

```{bash}
time zcat < ~/mimic/hosp/labevents.csv.gz | wc -l
```
For Uncompressed 

```{bash}
time wc -l ~/mimic/hosp/labevents.csv
```

This shows that compressed is slower than uncompressed. 

This is because compressed has to go through the decompressing of the file 

In other words, compressed saves storage, but slows time

Uncompressed takes up storage, but is faster. 